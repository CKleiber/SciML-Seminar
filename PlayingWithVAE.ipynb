{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import other libraries\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import specific functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# import custom functions\n",
    "from interpolate import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformations (resize, to tensor, normalize)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
    "\n",
    "# import custom cartoon dataset\n",
    "\n",
    "dataset = datasets.ImageFolder(root='data/Cartoon/', transform=transform)\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(18, 3))\n",
    "for i in range(6):\n",
    "    ax[i].imshow(dataset[i][0].permute(1, 2, 0))\n",
    "    #ax[i].axis('off')\n",
    "\n",
    "# make a dataloader and split into train and test sets\n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "test, val = train_test_split(test, test_size=0.5)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement convolutional variational autoencoder appropriate for the cartoon dataset\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, BOTTLENECK_SIZE=64, INPUT_SIZE=128):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.bottleneck = BOTTLENECK_SIZE\n",
    "        self.input_size = INPUT_SIZE\n",
    "\n",
    "\n",
    "        # construct en- and decoder\n",
    "        self.encoder = self.construct_encoder()\n",
    "        self.decoder = self.construct_decoder()\n",
    "\n",
    "        #define loss function and optimizer\n",
    "        self.mse_loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.mse_weight = 1000\n",
    "\n",
    "        # define optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-5)\n",
    "\n",
    "    def loss_function(self, x, x_hat, mu, log_var):\n",
    "        mse_loss = self.mse_loss_function(x_hat, x)\n",
    "        kl_loss = torch.mean(-0.5 * sum(1 + log_var - torch.square(mu) - torch.square(torch.exp(log_var))))\n",
    "\n",
    "\n",
    "        return self.mse_weight * mse_loss + kl_loss\n",
    "\n",
    "    def construct_encoder(self):\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(int(512*(self.input_size/(2**5))**2), 2 * self.bottleneck)\n",
    "        )\n",
    "\n",
    "        return encoder\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def construct_decoder(self):\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(self.bottleneck, int(512*(self.input_size/(2**5))**2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (512, int(self.input_size/(2**5)), int(self.input_size/(2**5)))),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        return decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=1)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "\n",
    "        # decode\n",
    "        x = self.decoder(z)\n",
    "        return x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop\n",
    "\n",
    "def train_vae(model, X_train, X_val, n_epochs=10, verbose=True):\n",
    "    loss_history = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        pbar = tqdm.tqdm(range(n_epochs))\n",
    "    else:\n",
    "        pbar = range(n_epochs)\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm.tqdm(X_train):\n",
    "            batch, _ = batch\n",
    "            model.optimizer.zero_grad()\n",
    "            output, mu, log_var = model(batch)\n",
    "            loss = model.loss_function(batch, output, mu, log_var)\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(X_train)\n",
    "        loss_history['train'].append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in X_val:\n",
    "                batch, _ = batch\n",
    "                output, mu, log_var = model(batch)\n",
    "                loss = model.loss_function(batch, output, mu, log_var)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= len(X_val)\n",
    "            loss_history['val'].append(val_loss)\n",
    "\n",
    "        #print loss\n",
    "        if verbose:\n",
    "            pbar.set_description('Epoch: {}/{}, train loss: {:.4f}, val loss: {:.4f}'.format(epoch+1, n_epochs, train_loss, val_loss))\n",
    "        \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to plot loss history\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    epochs_linspace = np.linspace(1, len(loss_history['train']), len(loss_history['train']))\n",
    "\n",
    "    ax.scatter(epochs_linspace, loss_history['train'], label='train loss', marker='o')\n",
    "    ax.scatter(epochs_linspace, loss_history['val'], label='val loss', marker='x')\n",
    "\n",
    "    ax.plot(epochs_linspace, loss_history['train'])\n",
    "    ax.plot(epochs_linspace, loss_history['val'])\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.grid(alpha=0.35)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and train\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    vae = VAE(BOTTLENECK_SIZE=200)\n",
    "    loss_history = train_vae(vae, trainloader, valloader, n_epochs=250)\n",
    "    torch.save(vae.state_dict(), 'vae.pt')\n",
    "    # save loss history\n",
    "    np.save('loss_history.npy', loss_history)\n",
    "\n",
    "else:\n",
    "    vae = VAE(BOTTLENECK_SIZE=200)\n",
    "    vae.load_state_dict(torch.load('vae.pt'))\n",
    "    loss_history = np.load('loss_history.npy', allow_pickle=True).item()\n",
    "\n",
    "plot_loss_history(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot original and reconstructions\n",
    "\n",
    "fig, ax = plt.subplots(2, 6, figsize=(18, 6))\n",
    "for i in range(6):\n",
    "    ax[0, i].imshow(testloader.dataset[i][0].permute(1, 2, 0))\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].imshow(vae(testloader.dataset[i][0].unsqueeze(0))[0].squeeze().detach().permute(1, 2, 0))\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do interpolation\n",
    "\n",
    "gif = interpolate(vae, np.random.normal(0, 1, (2, 200)), 0.2, 'interpolation_cartoon.gif', cartoon=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
