{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import other libraries\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import specific functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# import custom functions\n",
    "from interpolate import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mh:\\Uni\\Master\\Semester 2\\SciML\\SciML-Seminar\\PlayingWithVAE.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Uni/Master/Semester%202/SciML/SciML-Seminar/PlayingWithVAE.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# import celeb_a dataset and show example images\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/Uni/Master/Semester%202/SciML/SciML-Seminar/PlayingWithVAE.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mCelebA(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transform\u001b[39m=\u001b[39;49mtransforms\u001b[39m.\u001b[39;49mToTensor())\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Uni/Master/Semester%202/SciML/SciML-Seminar/PlayingWithVAE.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# show example images\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Uni/Master/Semester%202/SciML/SciML-Seminar/PlayingWithVAE.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m3\u001b[39m))\n",
      "File \u001b[1;32mh:\\Uni\\Master\\Semester 2\\SciML\\SciML-Seminar\\.venv\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:80\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[1;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtarget_transform is specified but target_type is empty\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload()\n\u001b[0;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_integrity():\n\u001b[0;32m     83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mh:\\Uni\\Master\\Semester 2\\SciML\\SciML-Seminar\\.venv\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:150\u001b[0m, in \u001b[0;36mCelebA.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mfor\u001b[39;00m (file_id, md5, filename) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_list:\n\u001b[1;32m--> 150\u001b[0m     download_file_from_google_drive(file_id, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_folder), filename, md5)\n\u001b[0;32m    152\u001b[0m extract_archive(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_folder, \u001b[39m\"\u001b[39m\u001b[39mimg_align_celeba.zip\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mh:\\Uni\\Master\\Semester 2\\SciML\\SciML-Seminar\\.venv\\Lib\\site-packages\\torchvision\\datasets\\utils.py:246\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[1;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[0;32m    243\u001b[0m         api_response, content \u001b[39m=\u001b[39m _extract_gdrive_api_response(response)\n\u001b[0;32m    245\u001b[0m     \u001b[39mif\u001b[39;00m api_response \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQuota exceeded\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 246\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    247\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe daily quota of the file \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m is exceeded and it \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be downloaded. This is a limitation of Google Drive \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand can only be overcome by trying again later.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n\u001b[0;32m    252\u001b[0m     _save_response_content(content, fpath)\n\u001b[0;32m    254\u001b[0m \u001b[39m# In case we deal with an unhandled GDrive API response, the file should be smaller than 10kB and contain only text\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later."
     ]
    }
   ],
   "source": [
    "# import celeb_a dataset and show example images\n",
    "\n",
    "data = datasets.CelebA(root='data', split='train', download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# show example images\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(data[i][0].permute(1, 2, 0))\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement convolutional variational autoencoder appropriate for the celeba dataset\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, BOTTLENECK_SIZE=64):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.bottleneck = BOTTLENECK_SIZE\n",
    "\n",
    "        # construct en- and decoder\n",
    "        self.encoder = self.construct_encoder()\n",
    "        self.decoder = self.construct_decoder()\n",
    "\n",
    "        #define loss function and optimizer\n",
    "        self.mse_loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.mse_weight = 1000\n",
    "\n",
    "        # define optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def loss_function(self, x, x_hat, mu, log_var):\n",
    "        mse_loss = self.mse_loss_function(x_hat, x)\n",
    "        kl_loss = torch.mean(-0.5 * sum(1 + log_var - torch.square(mu) - torch.square(torch.exp(log_var))))\n",
    "        return mse_loss + kl_loss\n",
    "\n",
    "    def construct_encoder(self):\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 256, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 2 * self.bottleneck)\n",
    "        )\n",
    "        return encoder\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def construct_decoder(self):\n",
    "        decoder = nn.Sequential(\n",
    "            nn.Linear(self.bottleneck, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 1, 1)),\n",
    "            nn.ConvTranspose2d(256, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        return decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=1)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "\n",
    "        # decode\n",
    "        x = self.decoder(z)\n",
    "        return x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop\n",
    "\n",
    "def train_vae(model, X_train, X_val, n_epochs=10, batch_size=32, verbose=True):\n",
    "    loss_history = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        pbar = tqdm.tqdm(range(n_epochs))\n",
    "    else:\n",
    "        pbar = range(n_epochs)\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i in tqdm.tqdm(range(0, X_train.shape[0], batch_size)):\n",
    "            batch = X_train[i:i+batch_size]\n",
    "            model.optimizer.zero_grad()\n",
    "            output, mu, log_var = model(batch)\n",
    "            loss = model.loss_function(batch, output, mu, log_var)\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= X_train.shape[0]\n",
    "        loss_history['train'].append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, X_val.shape[0], batch_size):\n",
    "                batch = X_val[i:i+batch_size]\n",
    "                output, mu, log_var = model(batch)\n",
    "                loss = model.loss_function(batch, output, mu, log_var)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= X_val.shape[0]\n",
    "            loss_history['val'].append(val_loss)\n",
    "\n",
    "        #print loss\n",
    "        if verbose:\n",
    "            pbar.set_description('Epoch: {}/{}, train loss: {:.4f}, val loss: {:.4f}'.format(epoch+1, n_epochs, train_loss, val_loss))\n",
    "        \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to plot loss history\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    epochs_linspace = np.linspace(1, len(loss_history['train']), len(loss_history['train']))\n",
    "\n",
    "    ax.scatter(epochs_linspace, loss_history['train'], label='train loss', marker='o')\n",
    "    ax.scatter(epochs_linspace, loss_history['val'], label='val loss', marker='x')\n",
    "\n",
    "    ax.plot(epochs_linspace, loss_history['train'])\n",
    "    ax.plot(epochs_linspace, loss_history['val'])\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "\n",
    "    #ax.set_yscale('log')\n",
    "\n",
    "    ax.grid(alpha=0.35)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
