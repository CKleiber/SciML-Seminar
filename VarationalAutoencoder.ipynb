{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define basic Variational Autoencoder and neessary routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variational autoencoder skeleton\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def _init__(self, input_size, num_hidden_layers, hidden_layer_size, bottleneck_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # define architecture of the vae\n",
    "        self.input_size = input_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.bottleneck_size = bottleneck_size\n",
    "        \n",
    "        # construct the encoder, reparameterise, and decoder\n",
    "        self.encoder = self.construct_encoder()\n",
    "        self.reparameterise = self.reparameterise()\n",
    "        self.decoder = self.construct_decoder()\n",
    "\n",
    "        #define loss function and optimizer\n",
    "        self.mse_loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.kl_loss_function = nn.KLDivLoss(reduction='mean')\n",
    "\n",
    "        self.mse_weight = 30\n",
    "        self.loss = self.loss_function()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def loss_function(self, x, x_hat, mu, log_var):\n",
    "        # loss function\n",
    "        mse_loss = self.mse_loss_function(x, x_hat)\n",
    "        kl_loss = self.kl_loss_function(mu, log_var)\n",
    "        return (self.mse_weight/(self.mse_weight + 1)) * mse_loss + (1//(self.mse_weight + 1)) * kl_loss\n",
    "\n",
    "    def construct_encoder(self):\n",
    "        # construct the encoder\n",
    "        encoder = nn.Sequential()\n",
    "\n",
    "        encoder.add_module('input', nn.Linear(self.input_size, self.hidden_layer_size))\n",
    "        encoder.add_module('relu_input', nn.ReLU())\n",
    "\n",
    "        for i in range(self.num_hidden_layers - 1):\n",
    "            encoder.add_module('hidden_' + str(i), nn.Linear(self.hidden_layer_size, self.hidden_layer_size))\n",
    "            encoder.add_module('relu_' + str(i), nn.ReLU())\n",
    "\n",
    "        # split output into mu and var components\n",
    "        encoder.add_module('mu', nn.Linear(self.hidden_layer_size, self.bottleneck_size))\n",
    "        encoder.add_module('log_var', nn.Linear(self.hidden_layer_size, self.bottleneck_size))\n",
    "\n",
    "        return encoder\n",
    "    \n",
    "    def reparameterise(self, mu, log_var):\n",
    "        # reparameterise\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def construct_decoder(self):\n",
    "        # construct the decoder\n",
    "        decoder = nn.Sequential()\n",
    "\n",
    "        decoder.add_module('input', nn.Linear(self.bottleneck_size, self.hidden_layer_size))\n",
    "        decoder.add_module('relu_input', nn.ReLU())\n",
    "\n",
    "        for i in range(self.num_hidden_layers - 1):\n",
    "            decoder.add_module('hidden_' + str(i), nn.Linear(self.hidden_layer_size, self.hidden_layer_size))\n",
    "            decoder.add_module('relu_' + str(i), nn.ReLU())\n",
    "\n",
    "        decoder.add_module('output', nn.Linear(self.hidden_layer_size, self.input_size))\n",
    "\n",
    "        return decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward pass\n",
    "        x = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(x, 2, dim=-1)\n",
    "        z = self.reparameterise(mu, log_var)\n",
    "        x = self.decoder(z)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop\n",
    "\n",
    "def train_vae(model, X_train, X_val, n_epochs=10, batchsize=32, verbose=True):\n",
    "    loss_history = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        pbar = tqdm.tqdm(range(n_epochs))\n",
    "    else:\n",
    "        pbar = range(n_epochs)\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i in tqdm.tqdm(range(0, X_train.shape[0], batchsize)):\n",
    "            batch = X_train[i:i+batchsize]\n",
    "            model.optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = model.loss_function(output, batch)\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= X_train.shape[0]\n",
    "        loss_history['train'].append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm.tqdm(range(0, X_val.shape[0], batchsize)):\n",
    "                batch = X_val[i:i+batchsize]\n",
    "                output = model(batch)\n",
    "                loss = model.loss_function(output, batch)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= X_val.shape[0]\n",
    "            loss_history['val'].append(val_loss)\n",
    "\n",
    "        #print loss\n",
    "        if verbose:\n",
    "            pbar.set_description('Epoch: {}/{}, train loss: {:.4f}, val loss: {:.4f}'.format(epoch+1, n_epochs, train_loss, val_loss))\n",
    "        \n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
